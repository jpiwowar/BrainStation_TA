{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook\n",
    "This notebook is meant as a guideline for a mostly-live coding demo delivered to a class of introductory Data Science students. **If you try to execute the code in this notebook end-to-end, you are likely to be disappointed**.  If you expect correct grammar, complete sentences, and fully coherent comments throughout, you are likely to be disappointed _again_. The demo itself is intended to take 30 minutes; some items in this notebook may have to be omitted because I talk faster than I code.  But hey, ya gotta have goals.\n",
    "\n",
    "Oh, and did I mention \"Don't try to the code inside the notebook as-is?\"\n",
    "\n",
    "Don't try to run the code inside the notebook as-is. \n",
    "\n",
    "No, really."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction/motivation\n",
    "Our goals are: \n",
    "* Show a simple example of executing python code from the command line, since much of our class work to date has been in Jupyter notebooks\n",
    "* Demonstrate tools/techniques that may be useful during work on capstone projects for parsing wb pages and retrieving hosted datafiles.\n",
    "* Demonstrate an incremental approach to building a script, making sure one piece works before movig on to the next\n",
    "\n",
    "## Data\n",
    "This is a Vancouver class, so for local flavor, we'll be using the data provided by [Mobi](https://mobibikes.ca) about their bikeshare system, at https://www.mobibikes.ca/en/system-data.  \n",
    "\n",
    "## Tools\n",
    "Our data page provides a list of links to files hosted on Google Drive.  In addition to some standard libraries we've seen before, we'll also be using: \n",
    "* requests ((for accessing the URL - more info at https://requests.readthedocs.io/en/master/user/quickstart)\n",
    "* Beautiful Soup (for exploring page content - https://www.crummy.com/software/BeautifulSoup/bs4/doc)\n",
    "* gdown (for downloading Google Drive files - https://github.com/wkentaro/gdown)\n",
    "\n",
    "## Setup\n",
    "We won't have time to walk through the environment setup for this, but I'll provide an environment export file for later.  Most of the packages we need, except ```gdown```, are supplied in the base Anaconda install.  For a fresh virtualenv, you'll need to install: \n",
    "* requests\n",
    "* lxml\n",
    "* bs4 \n",
    "* gdown (via conda-forge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "Before we do anything else, let's import the libraries we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import gdown \n",
    "import requests\n",
    "import sys "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing arguments\n",
    "Our script will take a simple single argument, the URL for the page containing our data.  It could be enough to read that argument this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_url=sys.argv[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but we want to make sure the argument is actually provided, so, we'll wrap this in a try/except block, which we saw in our earlier Python lessons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_url=''\n",
    "try:\n",
    "    page_url = sys.argv[1]\n",
    "except IndexError:\n",
    "    print('No URL provided')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(jpdemo) jp: Scripting % python sou.py\n",
    "No URL provided\n",
    "(jpdemo) jp: Scripting % python sou.py https://github.com\n",
    "(jpdemo) jp: Scripting % #Better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading URL\n",
    "The next step is to open the URL we've passed into the script.  We'll start out with the try/except this time, since we definitely want to catch things if they break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_req=''\n",
    "print(f'Attempting to access {page_url}') \n",
    "try:\n",
    "    page_req=requests.get(page_url)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f'Error accessing {page_url}:\\n {e} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, start with some tests.  We expect an error from the first, no output (clean execution) from the second:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(jpdemo) jp: Scripting % python sou.py http://www.all_good_sites_are_taken.com\n",
    "Attempting to access http://www.all_good_sites_are_taken.com\n",
    "Error accessing http://www.all_good_sites_are_taken.com:\n",
    " HTTPConnectionPool(host='www.all_good_sites_are_taken.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff390aede20>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')) \n",
    "(jpdemo) jp: Scripting % python sou.py https://github.com                     \n",
    "Attempting to access https://github.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try out the real URL!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(jpdemo) jp: Scripting % python sou.py https://www.mobibikes.ca/en/system-data \n",
    "Attempting to access https://www.mobibikes.ca/en/system-data\n",
    "Error accessing https://www.mobibikes.ca/en/system-data:\n",
    " HTTPSConnectionPool(host='www.mobibikes.ca', port=443): Max retries exceeded with url: /en/system-data (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1123)')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that ain't good. It looks like that the root SSL certificates in use by the Mobi site aren't in the default set used by our python environment.  While troubleshooting SSL issues is **super fun**, we don't really have the time, so we're going to take a shortcut.  This is not a good shortcut, but it does make the problem go away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_req=''\n",
    "print(f'Attempting to access {page_url}') \n",
    "try:\n",
    "    page_req=requests.get(page_url, verify=False) #Fix certs later. Demo purposes only.\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f'Error accessing {page_url}:\\n {e} ')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(jpdemo) jp: Scripting % python sou.py https://www.mobibikes.ca/en/system-data\n",
    "Attempting to access https://www.mobibikes.ca/en/system-data\n",
    "/Users/johnpiwowar/opt/anaconda3/envs/jpdemo/lib/python3.8/site-packages/urllib3/connectionpool.py:979: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.mobibikes.ca'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
    "  warnings.warn("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warnings are better than errors.  And the warning's a real one; this is not something to do in a real-world situation, since it creates security risks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the Soup\n",
    "Finally we can do what what we came to do. We'll create an instance of BeautifulSoup using the content of our web page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomato_basil=BeautifulSoup(page_req.content)\n",
    "# Tomato basil. It's a soup. Get it? Whee!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sou.py:20: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
    "\n",
    "The code that caused this warning is on line 20 of the file sou.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
    "\n",
    "  tomato_basil=BeautifulSoup(page_req.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the previous warning message, we can make this one go away pretty easily.  Depending on your environment, the parser you're using might need to change, but this will do for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomato_basil=BeautifulSoup(page_req.content,features='lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on, we can just choose which parts of our \"soup\" we want to display.  Something that they show early in the docs is how to get a well-formatted version of the whole page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tomato_basil.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we really want, though, are the links we saw before.  The quick way to get those is using find_all() on the anchor tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tomato_basil.find_all(\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also a shorthand version for using find_all when working with common elements like tags.  We'll use that from now on, because I hate typing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tomato_basil(\"a\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[<a class=\"element-invisible element-focusable\" href=\"#main-content\">Skip to main content</a>, <a class=\"logo navbar-btn pull-left\" href=\"/en\" title=\"Home\">\n",
    "<img alt=\"Home\" src=\"https://www.mobibikes.ca/sites/all/themes/smoove_bootstrap/logo.png\"/>\n",
    "</a>, <a href=\"/en/register\" title=\"\">Sign up</a>,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a good start, looks like we're getting a list of tags, and we can iterate through those.  To save ourselves a bit of scrolling while we focus on the output we want, we can also make use of the ```limit``` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for anchor in tomato_basil(\"a\",limit=3):\n",
    "    print(anchor)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<a class=\"element-invisible element-focusable\" href=\"#main-content\">Skip to main content</a>\n",
    "<a class=\"logo navbar-btn pull-left\" href=\"/en\" title=\"Home\">\n",
    "<img alt=\"Home\" src=\"https://www.mobibikes.ca/sites/all/themes/smoove_bootstrap/logo.png\"/>\n",
    "</a>\n",
    "<a href=\"/en/register\" title=\"\">Sign up</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we're processing anchors one at a time, which gets us clsoer to our goal of extracting the Google Drive URLs.  Small problem: There are anchors that aren't links for Google Drive.  We can use a regular expression to match the ```href``` attributes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_filter=re.compile('google.com')\n",
    "for anchor in tomato_basil(\"a\",href=link_filter,limit=3):\n",
    "    print(anchor)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<a href=\"https://drive.google.com/file/d/1-ADbyB1k1IIv3bjs70nG_m0C0zigI5n5/view?usp=sharing\">ALL of 2017</a>\n",
    "<a href=\"https://drive.google.com/file/d/135Jn8mMog5FccPf89KAtlu_vqpE2EHwX/view?usp=sharing\"> </a>\n",
    "<a href=\"https://drive.google.com/file/d/18nSmd8szs1abJ-_emITXkjPMhQ0sAA-k/view?usp=sharing\" target=\"_blank\">January 2018</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvement, but there's a \"hidden\" link in there with no text.  Fortunately, we can filter on that with a regex, too.  The text enclosed by a tag is in the tag's ```string``` attribute  We could filter just on the year (e.g. ```20\\d{2}```), but if the naming convention changes later, or if we want to use this script elsewhere, then that's a bit short-sighted.  Instead, let's filter out everything that only contains blanks, by requiring at least one non-blank character in ```string```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_filter=re.compile('google.com')\n",
    "string_filter=re.compile('\\S+') #small hack to skip \"invisible\" links\n",
    "for anchor in tomato_basil(\"a\",href=link_filter, string=string_filter, limit=3):\n",
    "    print(anchor)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<a href=\"https://drive.google.com/file/d/1-ADbyB1k1IIv3bjs70nG_m0C0zigI5n5/view?usp=sharing\">ALL of 2017</a>\n",
    "<a href=\"https://drive.google.com/file/d/18nSmd8szs1abJ-_emITXkjPMhQ0sAA-k/view?usp=sharing\" target=\"_blank\">January 2018</a>\n",
    "<a href=\"https://drive.google.com/file/d/18ZgxCe4Oo2Pus7HgHLecH9uuGtylwOaD/view?usp=sharing\" target=\"_blank\">February 2018</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're finally ready to start using ```gdown``` to get all the Google Drive files!  Let's see what happens if we pass the link directly to the ```gdown.download()``` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for anchor in tomato_basil(\"a\",href=link_filter, string=string_filter, limit=3):\n",
    "    gdown.download(anchor.get(\"href\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "/Users/johnpiwowar/opt/anaconda3/envs/jpdemo/lib/python3.8/site-packages/gdown/parse_url.py:28: UserWarning: You specified Google Drive Link but it is not the correct link to download the file. Maybe you should try: https://drive.google.com/uc?id=18ZgxCe4Oo2Pus7HgHLecH9uuGtylwOaD\n",
    "  warnings.warn("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so gdown won't take just _any_ Google Drive URL.  It expects a specific format. Ok, _fine_, what _ever_, geez Louise.   The warning message gives us a helpful hint about how the URL should look, and that's a pretty straightforward string replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for anchor in tomato_basil(\"a\", href=link_filter, string=string_filter):\n",
    "    download_link=anchor.get('href')\\\n",
    "        .replace('file/d/','uc?id=')\\ \n",
    "        .replace('/view?usp=sharing','')\n",
    "    gdown.download(download_link)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Downloading...\n",
    "From: https://drive.google.com/uc?id=1UYA5dLM_1Hkc3RQGnrGpu-GXJ70JZLm6\n",
    "To: /Users/johnpiwowar/Documents/gitrepos/BrainStation_TA/Scripting/Mobi_System_Data_2020-07.csv\n",
    "10.3MB [00:01, 8.84MB/s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Victory!  You'll note we also removed the ```limit=3``` from our ```for``` loop.  That's confidence. ;-) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "* Obviously, writing and testing this code took far longer than just manually clicking a bunch of links. In your capstone, however, or in your later jobs, you may find opportunities to use some of these tools to save yourself time, or just to have a repeatable non-clicky approach.\n",
    "\n",
    "* The basic argument parsing we did with ```sys.argv``` is ok for quick work, but there are more sophisticated methods (e.g. [argparse](https://docs.python.org/3/library/argparse.html) that can make your life easier and your code more readable.\n",
    "\n",
    "* This implementation is ok for a quick functionality demo, but if you were getting paid to write a command-line utility, you'd be expected to use some of the techniques we talked about in earlier lectures create a package.  One method for doing so is described [here](https://realpython.com/python-application-layouts/#one-off-script), but in general you can expect to follow your organization's standards.\n",
    "\n",
    "* Repeatedly executing \"python _my_script_ _my_parameters_ and printing results is ok for quick prototyping, but it's not a great way to test changes.  Back to the previous point, if you were being paid to write this code, you'd be expected to produce tests.  In fact, in many circumstances, you will benefit from writing some of your tests before writing any of the \"real\" code for your utility.  There will be more discussion of testing in later lectures.\n",
    "\n",
    "* As discussed earlier in class, you'll probably read differing opinions about \"scraping,\" which is what we're doing here.  This is admittedly small-scale stuff (a few requests spread out over half an hour), but even small things (e.g. accidentally executing this script 1000 times) can have big impacts.  When you're interacting with web pages like this, even if it seems harmless, keep in mind that you're doing things that might not fit the systems designer's intent. You're a guest: be careful, be polite...and remember that there are fellow geeks on the other end of the wire who are responsible for keeping their services healthy, and there's no point in making their lives difficult. :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
